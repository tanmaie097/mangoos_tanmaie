Parquet -> compressed, data is stored in columns instead of rows 
works with spark, hadoop, aws, snowflake 
saves space
loads faster

scala - everything is saved in objects 
allows functional programming
variable types are explicitly typed 

join 
pandas - is more for smaller datasets - laptop based, slow for bigdata
spark - larger, optimised speed for bigdata